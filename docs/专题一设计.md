# 编译原理实验一

[TOC]

### 前言

由于md对latex公式渲染和代码排版和高亮良好支持，我还是选用了md作为实验报告书写格式。并附上了实验报告的封面，将其导出成pdf和word。

Satori

### 一、目标任务

以下为正则文法所描述的 C 语言子集的但此符号的示例，请补充单词符号：++,--,>>,<<,+=,-=,*=,/=,&&,||,!等，给出补充后描述 C 语言子集单词符号的正则文法，设计并实现其词法分析程序。
<标识符>→字母 | <标识符>字母 | <标识符>数字
<无符号整数>→数字 | <无符号整数>数字
<单字符分界符>→+ | - | * | ; | , | ( | ) | { | }
<双字符分界符>→<大于>= | <小于>= | <小于>> | <感叹号>= | <等于>= | <斜竖>* 
<小于>→< 
<等于>→=
<大于>→>
<斜竖>→/
<感叹号>→!
要能够识别语言的保留字：void, int, float, double, if, else, for, do, while 等等（可以补充）

### 二、设计说明

#### 1、补充符号并给出正则文法

我参照C语言标准补充了一些单字符、双字符分隔符，以及所有的关键字：

```C++
std::set<std::string> keywords = {
	"auto", "break", "case", "char", "const", "continue", "default",
	"do", "double", "else", "enum", "extern", "float", "for", "goto",
	"if", "int", "long", "register", "return", "short", "signed",
	"sizeof", "static", "struct", "switch", "typedef", "union",
	"unsigned", "void", "volatile", "while"
};
```

补充后的正则文法：

<标识符>→字母<标识符 2>

<标识符 2>→字母<标识符 2> | 数字<标识符 2> | ε

<单字符分界符>→ + | - | * | / | % | = | ( | ) | { | } | [ | ] | < | > | ; | , | . | \ | & | ^ | | | ! 

<双字符分界符>→ ><等号> | <<等号> | ! <等号> | =<等号> | +<加号> | -<减号> | |<单字符或> | &<单字符与> | *<等号> | /<等号> | +<等号> | -<等号> | %< 等号> | <<小于号> | ><大于号>

<等号>→=

<加号>→+ 

<减号>→- 

<单字符或>→| 

<单字符与>→& 

<小于号>→< 

<大于号>→>

<有符号整数>→+<无符号整数> | -<无符号整数> | 数字<无符号整数>

<有符号浮点数>→+<无符号浮点数> | -<无符号浮点数> | 数字<无符号浮点数>

<行注释>→/<行注释 2> 

<行注释 2>→/<行注释 3>

<行注释 3>→任意不是换行符的字符<行注释 3> | 换行符

<块注释>→/<块注释 2> 

<块注释 2>→*<块注释 3> 

<块注释 3>→任意不是*的字符

<块注释 3> | *<块注释结束判定> 

<块注释结束>→/ | 任意不是/的字符<块注释 3>

#### 2、词法分析器调研

目前市面上有许多成熟的词法分析器生成器，例如

1. Lex：Lex是Unix系统上最常用的词法分析器生成器之一，它可以生成C语言的词法分析器。

2. Flex：Flex是Lex的一个开源替代品，它可以生成C/C++语言的词法分析器。

3. ANTLR：ANTLR是一款用Java编写的词法分析器生成器，它支持多种语言，包括Java、C#、Python等。

4. JFlex：JFlex是Flex的Java版本，它可以生成Java语言的词法分析器。

5. PLY：PLY是Python Lex-Yacc的缩写，它是Python语言的词法分析器和语法分析器生成器。

6. GNU Bison：GNU Bison是一个强大的语法分析器生成器，它可以生成C/C++语言的语法分析器。

其中，Lex是一个生成词法分析器的工具，它可以根据用户定义的正则表达式和对应的操作，自动生成C语言代码，用于对输入的字符串进行词法分析。下面是Lex的使用方法：

1. 定义词法规则：使用正则表达式定义输入字符串的模式，并为每种模式定义相应的操作。例如，以下代码定义了一个识别整数的规则，当匹配到整数时，将其转换为整型并返回。

```
[0-9]+    { return atoi(yytext); }
```

2. 编写Lex源文件：将所有的词法规则写入一个Lex源文件中，并在文件开头添加一些声明和定义，例如：

```
%{
#include <stdio.h>
// C语言声明，一般声明全局变量和函数，会复制进lex.yy.c中
%}

%option noyywrap

%%
// 规则段，每一行都是一条规则，每条规则由匹配模式和事件组成。
// 每当一个模式被匹配后，后面的事件被执行
{规则1}     { 操作1 }
{规则2}     { 操作2 }
...

%%
// 用户自定义的过程，会被复制到lex.yy.c的末尾
int main() {
    yylex();
    return 0;
}
```

其中，`%{` 和 `%}` 之间的代码将被直接复制到生成的C代码中，`%option noyywrap` 表示禁用yywrap函数，`%%` 表示词法规则的开始和结束，`main` 函数中的 `yylex()` 函数将会执行词法分析。

3. 使用Flex生成词法分析器：在命令行中输入 `flex lex_file.l` 命令，将会生成一个名为 `lex.yy.c` 的C语言源代码文件。

4. 编译并运行生成的代码：使用 `gcc -o lex_file lex.yy.c` 命令将C源文件编译为可执行文件，然后运行生成的可执行文件即可进行词法分析。

以上就是Lex的基本用法，通过定义词法规则和操作，可以快速生成一个词法分析器，用于对输入的字符串进行分析。

#### 3、大致思路

根据上面的调研，我想挑战一下自己，打算使用正则表达式（RE）来书写声明式的规范，使用NFA和DFA来自动构建词法分析器。

![image-20230416004741956](D:\课程文档\大三下\课程作业\编译\Lab1\assets\image-20230416004741956.png)

可以以下列五个元素的集合表示一个有限状态机：
$$
A = (S, \sum, \delta, s0, Fs)

S ：状态集合 \newline
\sum：输入的字母表（输入字符的集合） \newline
\delta：状态转移函数，\delta: S \times \sum \to S \newline
s0：初始状态，s0 \in S \newline
F：终止状态，F \subseteq S \newline

一个有限状态机从初始状态s0 出发，到终止状态 F 结束，中间透过输入字符c \in \sum和转移函数 \delta 决定状态的变化。
$$

经过进一步调研，梳理出大致思路如下：

根据实际需求，设计正则文法或正则式，将其转换为正则表达式；

使用 Thompson 算法将正则表达式转换为NFA；

使用子集构造算法将NFA确定化为DFA；

使用 Hopcroft 最小化算法得到最小化的DFA。

### 三、实验步骤

#### 3.0 实验思路浅析

#### 3.1 正则表达式的解析和处理

汤普森构造法是C语言&Unix之父之一的肯·汤普森(Ken Thompson)提出的构造识别正则表达式ε-NFA的方法，其原理非常简单，先构造识别子表达式的ε-NFA，再通过几个简单的规则将ε-NFA合并，最终得到识别完整正则表达式的ε-NFA。汤普森构造法的优点是构造速度快，且构造的ε-NFA状态数较少。

该方法首先需要设计识别并解析正则表达式的程序模块，关于识别正则表达式的具体的实现，大致有两种方法：

**方法一：**

基于对 RE 的结构做归纳，对基本的 RE 直接构造，对复合的 RE 递归构造。其优势是使用递归算法，相对比较容易实现。

**方法二：**

将RE转化为后缀表达式，利用栈结构完成对RE的解析。其优势是可以将复杂的表达式转化为能够依靠简单的操作得到计算结果的表达式。


经过斟酌，我决定使用第二种方法。
##### 3.1.1 正则表达式的介绍和约定

正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。

一般来讲，正则表达式支持如下特性：

> 1. 字符组：用方括号 [] 来表示，可以匹配方括号中的任意一个字符。例如 [abc] 可以匹配字符 a、b 或 c。
>
> 2. 数量词：用来指定匹配的次数，例如 * 表示匹配 0 次或多次，+ 表示匹配 1 次或多次，? 表示匹配 0 次或 1 次，{n} 表示匹配 n 次，{n,m} 表示匹配 n 到 m 次。
>
> 3. 特殊字符：包括元字符、转义字符和定位符。元字符包括 .、|、() 等，用来表示特定的字符或字符集合。转义字符用来表示特殊字符，例如 \d 表示匹配数字，\s 表示匹配空白字符。定位符用来匹配字符串的位置，例如 ^ 表示匹配字符串的开头，$ 表示匹配字符串的结尾。
>
> 4. 分组：用圆括号 () 来表示，可以将多个字符组合成一个整体，方便进行匹配和替换。例如 (ab)+ 表示匹配一个或多个连续的 ab。
>
> 5. 前后查找：用来匹配某个字符前面或后面的字符。例如 (?<=a)b 表示匹配前面是 a 的 b，(?=a)b 表示匹配后面是 a 的 b。
>
> 6. 贪婪匹配和非贪婪匹配：默认情况下，正则表达式会尽可能地匹配更多的字符，这种匹配方式称为贪婪匹配。如果在量词后面加上 ?，则表示非贪婪匹配，即尽可能少地匹配字符。

在本次实验过程中，我会实现一个**简易的正则表达式的解析器**，其所支持的功能是标准正则表达式功能的**子集**，具体功能约定如下（命名并不规范，仅作理解用）：

1. **字符组、字符区间、反选字符**：用方括号`[]`表示字符组，匹配方括号中的**任意**一个字符。形如`a-z`、`0-9`的字符子串表达一个字符**区间**。左方括号后紧跟一个`^`表达对字符组所确定的字符进行**反选**，即有效字符的**全集**减去字符组内的字符得到的新字符组。
2. **转义字符、等价字符、通配符**：以转义字符`\`开头的字符将会被转义，例如操作符`+`被转移后就失去了操作符的含义，可用于表达对加号的匹配。特别的，`\e`等价于符号$\epsilon$，`\d`等价于`0-9`，`\a`等价于`a-zA-Z`，`\w`等价于`a-zA-Z0-9_`，`\s`代表所有的空白字符，等等。`.`代表通配符，即接受有效字符的全集。
3. **单目操作符**：我实现的简易的正则表达式支持四种基本的单目操作符（均只接受**左目参数**）。`^`代表连接符（省略不写，分析程序会自动加上，无需转义）；`?`代表可选符，其之前的字符可以出现一次或零次；`+`代表正闭包，其之前的字符应至少出现一次；`*`代表闭包符，其之前的字符可以出现零次、一次或多次。
4. **双目操作符**：目前仅支持一种双目操作符`|`，表示该操作符两边的内容是或的关系，该操作符相比上述操作符的**优先级最低**。
5. **分组**：支持使用`()`提升圈中部分的字符的处理优先级，被圈中的字符集会被视为一个整体参与后续的计算。

控制字符和有效字符全约定

首先定义在解析正则表达式的过程中可能用到的特殊字符和控制字符：

```C++
enum Operator
{
	STAR = 0x01,	 // *
	PLUS = 0x02,	 // +
	QUES = 0x03,	 // ?
	SELECT = 0x04,	 // |
	CONCAT = 0x05,	 // ^
	PARENT_L = 0x06, // (
	PARENT_R = 0x07, // )
};

enum Symbol
{
	EPSILON = 0x00,	 // \e
	SET = 0x10,		 // []
	WILDCARD = 0x11, // .
	ALPHA = 0x12,	 // \a
	NUM = 0x13,		 // \b
	WORD = 0x14,	 // \w
	SPACE = 0x15,	 // \s
};
```

定义有限状态自动机如下（可以同时表达NFA和DFA）：

```C++
class FiniteAutomaton
{
	struct State
	{
		int id;
		bool isFinal;
		State(int id, bool isFinal) : id(id), isFinal(isFinal) {}
	};

	std::vector<State> states;
	std::unordered_map<int, std::unordered_map<char, std::vector<int>>> transitions;

public:
	FiniteAutomaton() {}
	int addState(bool isFinal);
	void setState(int id, bool isFinal);
	void addTransition(int from, int to, char symbol);
	bool accepts(std::string word);
	void print();
	bool match(std::string text, int idx, std::string &result);

	std::vector<State> getStates()
	{
		return states;
	}

	std::unordered_map<int, std::unordered_map<char, std::vector<int>>> getTransitions()
	{
		return transitions;
	}
};
```

定义一个负责从正则表达式生成FA的转换器类如下：

```C++
class Regexp2FA
{
	std::string rawReg;
	std::string tmpReg;
	std::string postfix;
	std::stack<int> stateStack;
	std::vector<int> setStates;
	FiniteAutomaton nfa;

	void parseSet(std::string setExp);

	void opSelect();
	void opStar();
	void opPlus();
	void opQues();
	void opConcat();

public:
	Regexp2FA(std::string regexp) : rawReg(regexp) {}
	void regexpPreproc();
	void tmpReg2postfix();
	void postfix2FA();
	std::string getRawReg() { return rawReg; }
	std::string getTmpReg() { return tmpReg; }
	std::string getPostfix() { return postfix; }
	FiniteAutomaton getNFA() { return nfa; }
	FiniteAutomaton convert();
};
```

转换过程大致分为三个步骤。



##### 3.1.2 正则表达式的预处理

**步骤一：对输入的正则表达式进行预处理，处理其中的特殊字符（如转义字符、操作字符等）**

例如，对于正则表达式：

```reStructuredText
(\-|\+|\e)[0-9]+
```

预处理器会将其转换为：

`PARENT_L`-`SELECT`+`SELECT` `EPSILON` `PARENT_R` `CONCAT` `SET` `PLUS`

这一步将操作字符用先前定义的枚举值代替，并将转义字符转化为真实代表的字符。同时预处理器会将诸如`[0-9a-z_]`的语法先行解析，并预先存储到`setStates`中备用。为了方便起见，我额外加入了`CONCAT`连接符。经过上述处理，正则表达式变成了一个方便进一步处理的**中缀表达式**。

##### 3.1.3 中缀表达式转后缀表达式

**步骤二：对上一步得到的中缀表达式进行处理，将其转换为后缀表达式**

例如，对于正则表达式：

```
>=|<=|!=|==|\+\+|\-\-|\|\||&&|\*=|/=|\+=|\-=|%=|<<|>>
```

经过步骤一处理得到：

`CONCAT`=`SELECT`<`CONCAT`=`SELECT`!`CONCAT`=`SELECT`=`CONCAT`=`SELECT`+`CONCAT`+`SELECT`-`CONCAT`-`SELECT`|`CONCAT`|`SELECT`&`CONCAT`&`SELECT`*`CONCAT`=`SELECT`/`CONCAT`=`SELECT`+`CONCAT`=`SELECT`-`CONCAT`=`SELECT`%`CONCAT`=`SELECT`<`CONCAT`<`SELECT`>`CONCAT`>

进一步经过步骤二处理得到：

\>=`CONCAT`<=`CONCAT` `SELECT`!=`CONCAT` `SELECT`==`CONCAT` `SELECT`++`CONCAT` `SELECT`--`CONCAT` `SELECT`||`CONCAT` `SELECT`&&`CONCAT` `SELECT`*=`CONCAT` `SELECT`/=`CONCAT` `SELECT`+=`CONCAT` `SELECT`-=`CONCAT` `SELECT`%=`CONCAT` `SELECT`<<`CONCAT` `SELECT`>>`CONCAT` `SELECT`

具体算法的实现：

连接符的插入

单目运算符、双目运算符和非运算符

#### 3.2 利用汤普森方法构造NFA

##### 3.2.1 汤普森构造法浅析

##### 3.2.2 后缀表达式的处理

**步骤三：对于得到的后缀表达式，利用栈的处理方法结合汤普森构造法得到最终的NFA**

部分执行过程截图如下：

![image-20230416234823336](D:\课程文档\大三下\课程作业\编译\Lab1\assets\image-20230416234823336.png)

得到如下所示的NFA：

![image-20230416234900400](D:\课程文档\大三下\课程作业\编译\Lab1\assets\image-20230416234900400.png)

具体代码不再赘述，详情请查看源码。



#### 3.3 词法分析器的简单实现

##### 3.3.1 词法分析器的规则定义



##### 3.3.2 对单条规则的解析处理

为NFA添加`accepts`函数，负责检测传入的字符串是否能被接受（非完全版）：

终止条件的分析，贪婪性，循环和早停

关于匹配策略的简单讨论和实现（最长前缀匹配和完全匹配）

词法分析器的分类匹配策略和错误处理策略

```C++
bool FiniteAutomaton::accepts(std::string word)
{
	int currentState = 0; // 初始状态为0
	for (char c : word)
	{
		auto it = transitions.find(currentState);
		if (it == transitions.end() || it->second.find(c) == it->second.end())
		{
			// 如果当前状态不存在对应字符的转移，则无法继续匹配
			return false;
		}
		// 执行状态转移
		currentState = it->second[c][0];
	}
	return states[currentState].isFinal;
}
```

##### 3.3.3 多条词法分析规则的处理

给出简单的词法分析程序示例如下：

```C++
void lexerTest()
{
	std::string regIdentifier = "[a-zA-Z][a-zA-Z0-9]*";
	std::string regSingleSeparator = "[\\+\\-\\*\\\\%=\\(\\){}\\[\\]<>;,.\\|&^!]";
	std::string regDoubleSeparator = ">=|<=|!=|==|\\+\\+|\\-\\-|\\|\\||&&|\\*=|/=|\\+=|\\-=|%=|<<|>>";
	std::string regSignedNumber = "(\\-|\\+|\\e)[0-9]+";
	std::string regSignedFloat = "(\\-|\\+|\\e)[0-9]+\\.[0-9]+";
	FiniteAutomaton faIdentifier = Regexp2FA(regIdentifier).convert();
	FiniteAutomaton faSingleSeparator = Regexp2FA(regSingleSeparator).convert();
	FiniteAutomaton faDoubleSeparator = Regexp2FA(regDoubleSeparator).convert();
	FiniteAutomaton faSignedNumber = Regexp2FA(regSignedNumber).convert();
	FiniteAutomaton faSignedFloat = Regexp2FA(regSignedFloat).convert();
	std::string sourceFilePath = "test.txt";
	// 读入文件
	std::ifstream file(sourceFilePath);
	std::string sourceCode = "";
	if (file.is_open())
	{
		std::string line;
		while (getline(file, line))
		{
			sourceCode += line;
		}
		file.close();
	}
	else
	{
		std::cout << "Unable to open file";
	}
	// 词法分析
	std::vector<std::pair<std::string, std::string>> tokens;
	int i = 0;
	while (i < sourceCode.length())
	{
		std::string token = "";
		if (faIdentifier.match(sourceCode, i, token))
		{
			if (keywords.find(token) != keywords.end())
			{
				tokens.push_back(std::make_pair("keyword", token));
			}
			else
			{
				tokens.push_back(std::make_pair("identifier", token));
			}
		}
		else if (faSingleSeparator.match(sourceCode, i, token))
		{
			tokens.push_back(std::make_pair("singleSeparator", token));
		}
		else if (faDoubleSeparator.match(sourceCode, i, token))
		{
			tokens.push_back(std::make_pair("doubleSeparator", token));
		}
		else if (faSignedNumber.match(sourceCode, i, token))
		{
			tokens.push_back(std::make_pair("signedNumber", token));
		}
		else if (faSignedFloat.match(sourceCode, i, token))
		{
			tokens.push_back(std::make_pair("signedFloat", token));
		}
		else
		{
			std::cout << "invalid token at " << i << std::endl;
			i++;
		}
	}
	// 输出结果
	for (auto token : tokens)
	{
		std::cout << token.first << " " << token.second << std::endl;
	}
}
```

##### 3.3.4 错误处理

#### 3.4 一些细节的说明和讨论

##### 3.4.1 分级日志的设计实现

##### 3.4.2 字符串浏览器的设计实现

##### 3.4.3 匹配策略的相关思考

DFA可以确保匹配最长的可能的字符串（最长前缀），但不能捕获子表达式。相应的，NFA一般采取贪婪的匹配回溯策略，以正则表达式指定的顺序测试所有可能的扩展并接受最长的匹配项。

##### 3.4.4 NFA和DFA的相关思考

在最坏的情况下，NFA回溯可能产生极深的递归，并导致速度的显著下降

### 四、测试用例

由于我选择了相对复杂的路径，截至实验提交日期前，我仍未能解决所有的bug，程序未能完全跑通。非常抱歉！我会在下一次实验报告给出相关内容，敬请见谅！



### 五、实验总结

本次专题实验，我编写了一个正规式转换为 NFA 的程序，将能够识别 C 语言词法的正规式转换为 NFA，用于扫描器进行词法分析。



### 六、代码说明

#### 6.1 代码开源

本次实验及后续实验所涉及到的代码均共享同一个项目框架，且源码开源在Github。

Github开源链接：https://github.com/Kogler7/SatoriCompiler

#### 6.2 代码结构

根目录

- `assets`：资源文件夹，例如测试用的源代码，以及词法分析器、语法分析器（目前还没有）的配置文件
- `docs`：说明文档，目前主要放实验报告及相关截图
- `src`：源代码文件夹
  - `codegen`：目标代码生成模块（暂时还没有）
  - `lexer`：词法分析器模块（本次实验主角）
    - `lexdef`：解析词法分析器配置文件的模块
    - `lexer`：词法分析器主要逻辑
    - `nfa`：NFA的实现
    - `regexp`：正则表达式解析模块
    - `viewer`：代码浏览器，方便其他模块进行解析
  - `parser`：语法分析器模块（暂时还没有）
  - `utils`：模块间共享的一些工具模块
  - `main.cpp`：程序主入口文件
- `CMakeList`：CMake说明
- `LICENSE`：开源协议（MIT）
- `README.md`：其他说明
