# 开悟编译器项目简要总结说明

[TOC]

## 1 写在前面

### 1.1 立项缘由

这个编译器的名字是`SatoriCompiler`。

`Satori`，意为**开悟、顿悟**。我为自己的编译器赋予此名，是期望自己能在动手实践的过程中不断地收获在**知识技能上的顿悟**，以及在**人生方向上的开悟**。`Satori`其实是一个**系列**，是我**大学实践探索的轨迹**。类似的，我还写了`SatoriOS`（基于龙芯的微型**操作系统**）和`SatoriBlog`（Vue+Spring+MySQL）的个人论坛**博客系统**。

在大学学习的方向上，我推崇**实践导向**和**问题导向**，认为**只有在不断实践中才能发现有价值的问题，只有在解决问题的过程中才能真正地学到知识和技能**。同时，我坚信，**量变才能产生质变**，只有一定的代码量的积累，才能真正培养出**利用计算机解决实际问题**的能力。编译器作为公认的编程届**三大浪漫**（分别是操作系统、编译器和计算机图形学）之一，又作为专业课程中的**核心一环**，必然蕴含了大量值得学习研究的思想和方法。因此，我认为自己动手实现一个小型的编译器非常有意义，这也必将是我求知道路上具有**里程碑意义**的一件大事。

### 1.2 项目定位

我要做的是一个**独立**的、**有效**的编译器，既不简单，也不复杂。

不简单，是指**所有代码均自主编写**，不依赖除STL以外的库和工具。在这个项目中，我常用的**格式化日志输出工具、自动表格排版工具、元信息解析工具和输入流浏览工具**均是我手动编写的。同时我要求这个编译器必须有足够的**抽象性**和**鲁棒性**。所谓抽象性，即是编译器要能提供尽可能高的抽象，允许用户自定义词法、语法乃至语义，编译器要能根据用户给出的**正则表达式**定义的词法自动生成基于NFA的词法分析器，要能根据**EBNF**定义的文法自动生成语法分析器（也就是老师说的**自动造表**），编译器要将语法分析得到的**CST**转化为简洁清晰的**AST**，以便对语义分析部分隐去不必要的细节，编译器要生成基于**LLVM IR**的中间表示（可以理解为三地址码的升级版），这样便可借助LLVM工具链来验证编译器前端生成结果的正确性，也可以利用LLVM后端实现代码优化，使得编译器真正具有**实用价值**。

不复杂，指的是要**综合投入成本和时间效率**的考虑，将实现重心放在编译器的**前端**，同时完成老师布置的实践作业。由于时间原因，编译器很难做到良好的**错误提示**和**代码优化**，这部分工作可以放到暑假参与编译竞赛的过程中去做。

### 1.3 开发环境

项目使用`C++20`标准（低于此标准可能导致编译错误），基于`MSVC（Microsoft Visual C++）`编译器（如果使用GCC编译可能出现不兼容问题）。项目使用`CMake`工具辅助工程构建。项目开发使用`Visual Studio Code`和`Visual Studio 2022`，其中`VSCode`是开发主力，`VS 2022`则主要在测试调试时使用。项目接受文件输入，产生命令行输出。

## 2 项目架构

### 2.1 元信息的定义和解析器实现

在我的项目中，一切**配置文件**都遵循**元信息**的格式。我定义的元信息的基础格式如下：

```
#meta PATTERN ${ $}
#meta IGNORED ${ $}

PATTERN ${
    BLANK       \s+
    LIN_CMT     //[^\r\n]*
    BLK_CMT     /\*([^\*]|\*[^/])*\*/
    IDENTIFIER  [\a_][\w]*
    SEPARATOR   [\+\-\*\\\(\)\[\]/,;=]
$}

IGNORED ${
    BLANK
    LIN_CMT
    BLK_CMT
$}
```

这是一个**词法**定义文件，文件中定义了两个元信息`#meta PATTERN`和`#meta IGNORED`，并将信息的内容通过其后定义的**起止符**包裹起来。这样的文件可以被我实现的`MetaParser`解析，从而作为后序程序的配置输入。

元信息的功用远不止于此。事实上，我为`MetaParser`添加了**递归下降**的实现方式，这使得元信息之中还可以**嵌套元信息**。同时，元信息不但支持含有起止符标记的**块信息**的解析，还支持由引导符标记的**行信息**的解析。一个典型的应用实例如下：

```
/**
* Extended Syntax Definition for the Reduced Language for Satori Compiler
* (c) 2023 Satori Compiler Project, Beijing Jiaotong University
*/

#meta GRAMMAR ${ $}
#meta MAPPING ${ $}
#meta SEMANTIC :

GRAMMAR ${
    Program*    ::=     { VarDeclStmt | FuncDeclStmt | FuncDef }   : SemProgram
                    ;
    VarDeclStmt ::=     VarDecl `;`                                : SemVarDeclStmt
                    ;
                    ......
$}
......
```

上面是从**RSC**（我计划实现的语言的名称）**文法**定义中截取的一部分，可以看到，我利用**嵌套的行信息**在EBNF中为对应的产生式添加了**语义动作**的注解。

在我的项目中，**几乎所有**需要配置的地方都接受元信息的输入，一个典型的例子如下：

```C++
/**
 * @brief Syntax Parser的构造函数，主要完成用到的词法分析器的初始化
 *
 * @param ebnfLexPath EBNF词法分析器的元数据文件路径
 */
SyntaxParser::SyntaxParser(const string ebnfLexPath)
{
    MetaParser lexMeta = MetaParser::fromFile(ebnfLexPath);
    ebnfLexer = Lexer(lexMeta["EBNF"], lexMeta["IGNORED"]);
    mappingLexer = Lexer(lexMeta["MAPPING"], lexMeta["IGNORED"]);
    precLexer = Lexer(lexMeta["PREC"], lexMeta["IGNORED"]);
}
```

元信息解析器并不是我一开始就设计实现的，而是在不断迭代完善整个编译器的过程中才出现的需求。相关的实现细节请参考源码`src/utils/meta`，在此不再赘述。

### 2.2 基于正则引擎的词法分析器

此节已在**第一次实验报告**中详细讨论过，这里只挑一些重点阐述。

#### 2.2.1 词法规则的定义

定义词法有多种方式，课上老师主要讲的是**正则文法**。我**采用的是正则表达式**，这主要是受到现代比较成熟的**词法/文法分析器生成器**的做法的启发，有其工程实践上的优势和意义。针对这样的设计，我实现了一个**正则表达式解析引擎**和**NFA构造器**，同时根据文法定义完成了词法分析结果到相应关键字、终结符的**映射**。这样做有许多好处，下面将进行讨论。

正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。

一般来讲，正则表达式支持如下特性：

> 1. 字符组：用方括号 [] 来表示，可以匹配方括号中的任意一个字符。例如 [abc] 可以匹配字符 a、b 或 c。
> 2. 数量词：用来指定匹配的次数，例如 * 表示匹配 0 次或多次，+ 表示匹配 1 次或多次，? 表示匹配 0 次或 1 次，{n} 表示匹配 n 次，{n,m} 表示匹配 n 到 m 次。
> 3. 特殊字符：包括元字符、转义字符和定位符。元字符包括 .、|、() 等，用来表示特定的字符或字符集合。转义字符用来表示特殊字符，例如 \d 表示匹配数字，\s 表示匹配空白字符。定位符用来匹配字符串的位置，例如 ^ 表示匹配字符串的开头，$ 表示匹配字符串的结尾。
> 4. 分组：用圆括号 () 来表示，可以将多个字符组合成一个整体，方便进行匹配和替换。例如 (ab)+ 表示匹配一个或多个连续的 ab。
> 5. 前后查找：用来匹配某个字符前面或后面的字符。例如 (?<=a)b 表示匹配前面是 a 的 b，(?=a)b 表示匹配后面是 a 的 b。
> 6. 贪婪匹配和非贪婪匹配：默认情况下，正则表达式会尽可能地匹配更多的字符，这种匹配方式称为贪婪匹配。如果在量词后面加上 ?，则表示非贪婪匹配，即尽可能少地匹配字符。

我实现的**简易的正则表达式的解析引擎**，其所支持的功能是标准正则表达式功能的**子集**，具体功能约定如下（命名并不规范，仅作理解用）：

1. **字符组、字符区间、反选字符**：用方括号 `[]`表示字符组，匹配方括号中的**任意**一个字符。形如 `a-z`、`0-9`的字符子串表达一个字符**区间**。左方括号后紧跟一个 `^`表达对字符组所确定的字符进行**反选**，即有效字符的**全集**减去字符组内的字符得到的新字符组。
2. **转义字符、等价字符、通配符**：以转义字符 `\`开头的字符将会被转义，例如操作符 `+`被转移后就失去了操作符的含义，可用于表达对加号的匹配。特别的，`\e`等价于符号$\epsilon$，`\d`等价于 `0-9`，`\a`等价于 `a-zA-Z`，`\w`等价于 `a-zA-Z0-9_`，`\s`代表所有的空白字符，等等。`.`代表通配符，即接受有效字符的全集。
3. **单目操作符**：我实现的简易的正则表达式支持四种基本的单目操作符（均只接受**左目参数**）。`^`代表连接符（省略不写，分析程序会自动加上，无需转义）；`?`代表可选符，其之前的字符可以出现一次或零次；`+`代表正闭包，其之前的字符应至少出现一次；`*`代表闭包符，其之前的字符可以出现零次、一次或多次。
4. **双目操作符**：目前仅支持一种双目操作符 `|`，表示该操作符两边的内容是或的关系，该操作符相比上述操作符的**优先级最低**。
5. **分组**：支持使用 `()`提升圈中部分的字符的处理优先级，被圈中的字符集会被视为一个整体参与后续的计算。

正则表达式涉及到对ASCII字符串的解析，首先可以将ASCII字符大致分为两类，一类是供正则表达式引擎识别的**普通字符**，另一类则是表达正则表达式语义的**特殊字符**。其中，特殊字符主要包含**运算符**和**转义字符**。

#### 2.2.2 正则表达式的处理

我设计的正则表达式解析过程主要分为**三个阶段**。

**第一步，我的正则引擎首先会对正则表达式做预处理**。

这么做的最核心的原因是，将正则表达式转化为**整齐**的**中缀表达式**。所谓整齐，我给的定义就是指表达式中的所有**表达特定语义的单元**都必须是**单个字符**，以方便后续的解析处理。举例而言，正则表达式中范围选择语义`[0-9]`是多个字符，经过预处理后会变成一个不可见的单字符，这里用`SET`表示（预处理器会将诸如 `[0-9a-z_]`的语法先行解析，并预先存储到 `setStates`中备用）。类似的，`\e`要被处理为`EPSILON`，`\+`要被处理为`+`（因为不经转义的`+`会被识别为运算符）。

此外，为了方便后续处理，预处理器会根据字符的结合性在字符之间添加**连接符**`CONCAT`。

例如，对于正则表达式：

```reStructuredText
(\-|\+|\e)[0-9]+
```

预处理器会将其转换为：

`PARENT_L`-`SELECT`+`SELECT` `EPSILON` `PARENT_R` `CONCAT` `SET` `PLUS`

**第二步，我的正则引擎会将中缀表达式转为后缀表达式**。

这么做是因为逆波兰式更便于机器分析计算（使用**栈**即可）。

例如，对于正则表达式：

```
>=|<=|!=|==|\+\+|\-\-|\|\||&&|\*=|/=|\+=|\-=|%=|<<|>>
```

经过**预处理**得到：

.>`CONCAT`=`SELECT`<`CONCAT`=`SELECT`!`CONCAT`=`SELECT`=`CONCAT`=`SELECT`+`CONCAT`+`SELECT`-`CONCAT`-`SELECT`|`CONCAT`|`SELECT`&`CONCAT`&`SELECT`*`CONCAT`=`SELECT`/`CONCAT`=`SELECT`+`CONCAT`=`SELECT`-`CONCAT`=`SELECT`%`CONCAT`=`SELECT`<`CONCAT`<`SELECT`>`CONCAT`>

进一步转化为**后缀表达式**得到：

\>=`CONCAT`<=`CONCAT` `SELECT`!=`CONCAT` `SELECT`==`CONCAT` `SELECT`++`CONCAT` `SELECT`--`CONCAT` `SELECT`||`CONCAT` `SELECT`&&`CONCAT` `SELECT`*=`CONCAT` `SELECT`/=`CONCAT` `SELECT`+=`CONCAT` `SELECT`-=`CONCAT` `SELECT`%=`CONCAT` `SELECT`<<`CONCAT` `SELECT`>>`CONCAT` `SELECT`

**第三步，利用汤普森方法根据后缀表达式构造NFA**。

汤普森构造法是一种将正则表达式转换为NFA的方法。其基本思想是，将正则表达式中的每个字符或操作符转换为一个NFA，并通过连接和或操作符将它们组合起来。

具体步骤如下：

 1、对于正则表达式中的每个字符，构造一个只有两个状态的NFA。该NFA有一个转移边，标记为该字符。
 2、对于正则表达式中的连接（`CONCAT`）操作符，将前一个NFA的终止状态连接到后一个NFA的起始状态。
 3、对于正则表达式中的或操作符（`SELECT`），构造一个新的起始状态和一个新的终止状态，并将前一个NFA和后一个NFA分别连接到这两个状态。
 4、对于正则表达式中的闭包（`STAR`）操作符，构造一个新的起始状态和一个新的终止状态，并将原来的NFA连接到这两个状态。然后，将新的起始状态连接到原来的起始状态和新的终止状态，将原来的终止状态连接到原来的起始状态和新的终止状态。
 5、最终的NFA的起始状态为正则表达式的起始状态，终止状态为正则表达式的终止状态。

 <center>
 	<img src="https://img-blog.csdnimg.cn/img_convert/c19e51dab02f142d16b1210ce713575f.png" width="15%"></img>
 	<img src="https://img-blog.csdnimg.cn/img_convert/3bf9d5f046ce7566697d833f14fd1bd0.png" width="20%"></img>
 	<img src="https://img-blog.csdnimg.cn/img_convert/92d5f471bf9a9bfa86fc5e613848949d.png" width="20%"></img>
 	<img src="https://img-blog.csdnimg.cn/img_convert/1f371a8ed4399a9f7a69daa6ad4a0f32.png" width="25%"></img>
 </center>


类似的还可以实现`PLUS`、`QUES`等操作符，不再赘述。

通过以上步骤，我们就可以将正则表达式转换为一个NFA。

#### 2.2.3 词法分析器的简单实现

实现词法分析器有两个重要步骤。

**第一，是在上一步生成的NFA的基础上，实现针对单条规则NFA的`accepts`方法**，负责检测传入的字符串是否能被该NFA接受，并返回接受的单词内容。在我的设计实现中，该函数主要采取**递归调用**的方式，每次递归向前接受一个字符，并将接受的字符拼接到返回的结果中，若无法接受则回溯至可以接受的状态，直至找到**最长的匹配前缀**或者匹配失败。

**第二，将多条词法规则对应的NFA组合成一个词法分析器，实现`tokenize`方法**，由词法分析器决定选用哪个NFA来处理当前的字符输入流，并得到最终的词素序列。在这里，我的实现方法比较简单粗暴，核心思路就是针对当前输入流**依次（有先后顺序）**使用不同规则定义的NFA尝试匹配，并取**第一个的最长匹配结果**作为接受的`Token`。当然，针对标记为`IGNORED`类型的token，词法分析器会直接将其丢弃。

受篇幅约束，相关实现细节请查阅第一次实验报告或源码。

### 2.3 递归下降的EBNF文法解析器

在我的定义中，**文法解析器**不是**语法分析器**。语法分析器是指根据某一文法去分析输入的词素序列是否满足文法要求。这里的文法是抽象的概念。在工程实践中，需要将**文法**定义为某种**数据结构**，并将通过某种方式定义的**描述文法的文件**解析成相应的文法数据。完成这项工作的，就是本节要讨论的文法解析器。

#### 2.3.1 元信息的配置与词法解析

在 2.1 节已经指出，文法解析器接受元信息作为输入，并依次完成内部词法分析器的配置。想必您遇到的第一个疑问便是，为什么文法解析器中会有词法分析器？请参见下面关于文法解析器的类型声明：

```C++
class SyntaxParser
{
    MetaParser syntaxMeta;                // 元信息解析器
    Lexer ebnfLexer;                      // EBNF词法解析器
    Lexer mappingLexer;                   // 映射词法解析器
    Lexer precLexer;                      // 优先级词法解析器
    Grammar grammar;                      // 语法解析结果
    std::map<symbol_t, int> nonTermCount; // 非终结符计数器，用于程序自动生成唯一的新非终结符

    std::vector<tok_product_t> tokProducts; // 用于存储解析出的词素产生式
    std::vector<tok_product_t> segmentProduct(tok_product_t &product);
    void parseNonTrivialProducts(std::vector<tok_product_t> &tmp, const symbol_t &left, token_const_iter_t beginIt, token_const_iter_t endIt);

    void addSyntaxRules(const std::vector<token> &tokens);
    void addTokenMappings(const std::vector<token> &tokens);
    void addPrecAndAssoc();

public:
    SyntaxParser(const std::string syntaxLexPath);
    Grammar parse(const std::string grammarPath);
};
```

在我的项目中，文法由**EBNF**（Extended Backus-Naur Form，即扩展巴克斯-诺尔范式）定义，该范式在BNF范式的基础上，定义了几个方便的运算符，用以表达**分组、可选、重复**等语义。为了解析EBNF，我首先利用词法分析器将其转化为词素序列，以便后续分析。

针对于EBNF解析的词法分析的词法定义如下。其他类似。

```
EBNF ${
    BLANK       \s+
    EPSILON     \\e
    START_MRK   \*
    SEMANTIC    SEMANTIC
    TERMINAL    `[^`]*`
    NON_TERM    [\a_][\w']*
    MUL_TERM    $[\a_][\w']*
    TOK_TYPE    @[\a_][\w']*
    DELIMITER   [\(\){}\[\]\|]
    SEPARATOR   ;
    GRAMMAR_DEF ::=
    COMMENT     //[^\r\n]*
    COMMENT     /\*([^\*]|\*[^/])*\*/
$}
```

事实上，我的文法解析器的真正的输入是**词素序列**，包括**EBNF的词素序列、终结符映射定义的词素序列、优先级和结合性关系定义的词素序列**等等，他们分别遵循不同的词法，因此需要为文法解析器配备多个词法分析器。

关于终结符映射和优先级结合性的讨论，请参考 2.4.3 小节。

关于这部分的详细实现，请查阅源码。

#### 2.3.2 文法产生式定义的预处理

文法解析器首先会根据EBNF词素流提取出初步的产生式信息。在这个过程中得到的产生式并不是最终文法的产生式，而是包含**EBNF运算符**的**词素产生式**。

上面一段话包含两个信息。

**其一，预处理得到的词素产生式中包含EBNF运算符**。我定义的EBNF运算符有四种，分别是**`|`选择符、`()`分组符、`[]`可选符和`{}`重复符**。选择符分开的部分将分别变成单独的产生式加入到最终的文法中，分组符可以针对每个选择部分附加组外的其他部分，可选符意味着其中的部分可有可无，重复符意味着其中的部分会重复出现零次或多次。这些运算符在预处理阶段会被保留，等留到后续阶段再完成递归下降的解析处理。

下面是关于四种运算符的样例（摘自我自己设计的RSC文法）：

```ebnf
Program*    ::=     { VarDeclStmt | FuncDeclStmt | FuncDef }
VarType     ::=     `int` | `real` | `bool` | `char` | `str`
ParamList   ::=     Param { `,` Param }
UnaryExpr   ::=     ( `+` | `-` | `!` ) UnaryExpr
Stmt        ::=     `if` `(` BoolExpr `)` Stmt [ `else` Stmt ]
```

**其二，预处理得到的产生式中包含词素信息**。标准的产生式只包含终结符和非终结符，也就是源码中的`symbol`，而预处理器在这个阶段并没有将词素序列的附加信息删除。**词素中的附加信息包括，词素的类型和词素在源码中的位置**。这些信息有助于在文法解析出错时，程序可以向用户提示文法出错的上下文信息。

预处理阶段的主要工作就是根据文法定义中的分隔符`;`来完成产生式的初步拆分，详细实现请参照源码相关部分，在此不再赘述。

#### 2.3.3 递归下降的EBNF运算符解析

在我的实现中，EBNF解析器是递归下降完成解析的。这么做的根本原因是，我希望支持EBNF运算符的**嵌套**和**组合**。下面是一些典型的例子。

EBNF运算符组合：

```
VarDef ::= $Ident `:` VarType { `[` $Integer `]` } [ `=` InitVal ]
```

EBNF运算符嵌套：

```
InitVal ::= Expr | `{` [ InitVal { `,` InitVal } ] `}`
```

运算符解析的过程大致可以分为**两步**。

**第一步**，根据选择符`|`划分产生式，若发现其他运算符则对需要处理的部分**递归执行**第二步；

**第二步**，根据不同运算符执行相应处理，若发现运算符中包含选择符，则先对需要处理的部分**递归执行**第一步。

如此往复，就实现了EBNF运算符组合与嵌套的情况的解析。

从细节上来看，针对不同EBNF运算符的处理也值得讨论。

首先是针对**分组符**的处理。

文法解析器处理分组符的过程相对比较简单。它只需要将分组符所包含的内容送入上述第一步方法中，便可以得到拆分后的**子产生式向量**，而后将每一个子产生式与产生式剩余部分进行**组合**（实际上为**全连接**操作，详细原因不再赘述）即可。其处理方式可以描述如下：

```
S -> A(B|D)C => S -> ABC, S -> ADC
```

其次是针对**可选符**的处理。

可选符的处理与分组符类似，不同之处在于，文法解析器在处理可选符时需要额外构造一个**不含可选符中内容**的产生式。

```
S -> A[B|D]C => S -> ABC, S -> ADC, S -> AC
```

最后是针对**重复符**的处理。

处理重复符的过程本质上是**创建新的非终结符**，并构建**含右递归的新产生式**的过程。**程序自动构建**的新产生式将会用于**文法分析**，并在文法分析结束后通过**重构CST将其消去（将在 2.5.4 节介绍）**，最终在用户的视角来看，这一切都是**透明**的。

```
S -> A{B|D}C => S' -> B|D, S'' -> S'S'' | ε, S -> AS''C
```

#### 2.3.4 拓展语法信息的解析处理

拓展语法信息包含**终结符映射、优先级与结合性、属性文法语义动作**等信息。至于为什么需要这些信息，请参见 下面的讨论。

2.3.2 节说到，预处理阶段得到的产生式包含词素信息。经过 2.3.3 节的处理解析，各产生式基本拆分完毕。本节要做的工作，就是将词素产生式中多余的信息删去，构建出**文法实例**，同时完成拓展语法信息的解析。

解析器会首先遍历每一个词素产生式的每一个token，按照其类别将其转化为符号`symbol`，构建文法产生式，并同时生成文法的终结符、非终结符集合。这是一个**平凡**的过程，不赘述。在这个过程中，有**三个**需要注意的特别的地方。

**第一个是终结符映射信息的解析**。

在EBNF定义经过词法分析后，会产生一种名为**映射终结符**（在程序中一般命名为`mul-term`）的词素，它隐含了一种信息，即该类终结符将在语法分析阶段对应**含有多种不同的值的词素**，一种典型的映射终结符是`ident`（意为**标识符**），他将在语法分析阶段匹配多种类型为`IDENTIFIER`但值各有不同的词素。除了`ident`，还有一些常见的映射终结符，他们在项目文法中的定义如下：

```
MAPPING ${
    $Ident     -->     @IDENTIFIER ;
    $Integer   -->     @INTEGER ;
    $Real      -->     @REAL ;
    $String    -->     @STRING ;
$}
```

在上面的定义中，箭头`-->`的左边代表文法中出现的映射终结符，右边代表词法分析器解析得到的词素类型，下面分别给出文法和词法中使用映射非终结符的例子：

```
FuncDef     ::=     Type $Ident `(` [ ParamList ] `)` Block
```

```
IDENTIFIER  [\a_][\w]*
```

不难发现，映射终结符的存在是一种**必然**，因为程序代码需要支持**用户自定义符号**和**数据字面量**，而仅凭文法终结符是**无法穷举**所有可能的字符组合的，这便是映射终结符存在的意义。

上述解析大致的实现方法是，文法解析器会将带有`mul-term`类型标记的词素转化为普通的终结符，但同时填写存储在**文法数据结构**中的一个**映射表**。通过这个映射表，我们就可以在语法分析之前，先将源码遍历得到的**词素序列**做一次**映射翻译**，再将其送入语法分析器分析，这样就实现了映射。

**第二个是优先级与结合性信息的解析**。

优先级与结合性在老师讲授的文法分析的内容中并不是重点，但其在**工程实践上**意义重大。它们一方面是各类**程序语言中不可或缺的特性**，另一方面则是**解决文法二义性问题的工具**。

除了老生常谈的**表达式中优先级与结合性规则的应用**，还有一个经典的案例是`dangling else`问题。通过实践不难发现，常见的**if else**文法具有二义性。其最主要的特点是，语法分析器不知道**将else语句划分给哪一个if语句**，同时，如果文法设计的不好，该文法在SLR分析器中还会产生**移进-规约冲突**，即语法分析器在遇到`if stmt . else stmt`时不知道该移进还是该规约。这将大大限制了文法的表达能力和SLR1语法分析器的应用。

经过调研发现，现代语法分析器生成器（Yacc等）处理二义性文法的普遍做法分两步进行：

1. 按**优先级规则**消除部分移进/规约冲突

2. 再按**确定性规则**解决余下的冲突

​		a. 对于移进-规约冲突，优先移进
​		b. 对于规约-规约冲突，选用列在文法最前面的语法规则规约

当然，我们也可以选择修改文法。但这样会造成**文法规则的分散**，并为之后的**语义规则的绑定**带来额外的困难。

针对优先级的解决方案，有如下讨论

> BYacc：如果lookahead记号和栈顶符号都有优先级，选择优先级高的，如果优先级相同，则看结合性；右结合选择移进，左结合选择规约，无结合两者皆不选。如果两者不全有优先级，则优先移进（即根据确定性规则处理移进-规约冲突）
>
> CUP：如果栈顶符号有优先级，则比较它和lookahead的优先级，大于则移进，小于则规约，等于再看结合性；左结合则规约，右结合则移进，无结合则做特殊标记。如果栈顶符号没有优先级，lookahead有优先级，则移进。如果两者皆没有优先级，则用确定性规则解决。

在我的项目中，优先级和结合性通过前述元信息的方式定义，并由文法解析器解析后填入到文法数据结构中。

**第三个是属性文法语义动作的解析**。

在我的项目中，语义信息的标记如下所示：

```
GRAMMAR ${
    Program*    ::=     { VarDeclStmt | FuncDeclStmt | FuncDef }  : SemProgram
                    ;
    VarDeclStmt ::=     VarDecl `;`                               : SemVarDeclStmt
                    ;
    VarDecl     ::=     `var` VarDef { `,` VarDef }               : SemVarDecl
                    ;
$}
```

在进行文法解析的过程中，元信息解析器会将语义信息提取出来，形成一个**语义动作的序列**。文法解析器则负责在整理文法的过程中根据产生式所在的行为每个产生式附加其对应的**语义动作标记**。关于语义标记方面，我最开始采取的方案是利用**C++宏定义**来实现语义动作的自定义，但我很快发现这样做有两个巨大的缺陷。其一是宏定义会将参数中代码块中的逗号当作参数的分隔符，导致语义动作不能被正确定义；其二是通过宏定义定义的语义动作只能全局被导入一次，否则会出现重复定义的编译错误。因此，最后我丢弃了这个方案，改为采用语义动作标记加内置相关代码的方案。

### 2.4 文法和语法分析器及其扩展

#### 2.4.1 基础文法及其继承体系的设计与实现

这一节主要对标的是**实验内容**。实验要求实现LL(1)、OPG和SLR1语法分析器，我将文法和语法分析器分别抽象出来，每一种类型的分析器都接受其相应类型的文法，这样就形成了文法之间的继承体系。首先是基础文法。

> 本节所述文法相关代码均可在源码`src/common/gram`文件夹下找到。

```C++
class Grammar
{

public:
    symbol_t symStart;
    symset_t mulTerms;
    symset_t nonTerms;
    symset_t terminals;

    product_t startProduct;
    std::vector<product_t> products;
    std::map<symbol_t, std::set<symstr_t>> rules;

    std::map<token_type_t, symbol_t> tok2sym;
    std::map<product_t, semantic_t> semMap;
    std::map<symbol_t, prec_assoc_t> precMap;
    ......
};
```

为节约篇幅，上述代码隐去了一部分方法，**仅保留了类所包含的字段**，下面代码也将如此。观察上述代码不难发现，基础文法作为其他文法的基类，记录了由文法解析器解析EBNF得到的所有信息，包括**开始符号、终结符、非终结符、映射终结符、产生式、文法规则、映射关系表、语义动作表和优先级关系表**等等。

在此基础上，我们可以得到**预测文法**。所谓预测文法，可以理解为为**LL(1)语法分析器**服务的文法数据结构。其中包含了**First集、Follow集的计算方法和结果**。其实LL(1)分析法无论是利用**递归下降**分析，还是利用**显式的栈**（递归下降本质上是利用隐式的栈）进行分析，都要用到**预测分析表**，而计算First集和Follow集就是预测分析的**关键**所在。因此，该文法便得名预测文法。

预测文法的数据结构如下（这边我其实额外求了一个**select集**，会更便于判断一个文法是否是LL1文法，更便于填写预测分析表）：

```C++
class PredictiveGrammar : public Grammar
{
	......
public:
    std::map<symbol_t, symset_t> first;
    std::map<symstr_t, symset_t> firstS;
    std::map<symbol_t, symset_t> follow;
    std::map<product_t, symset_t> select;
    ......
};
```

**LR文法**继承了预测文法。在LR系列的分析法中，需要计算项目集规范族和分析表。在一些LR分析法中，需要**利用Follow集来处理冲突**，因此LR文法中需要提供计算First集和Follow集的方法，这便是LR文法需要继承自预测文法的原因。

```C++
class LRGrammar : public PredictiveGrammar
{
    ......
public:
    clusters_t clusters;
    std::vector<lr_item_t> items;
    table_t<state_id_t, symbol_t, state_id_t> goTrans;
    ......
};
```

在LR基础文法的基础上，SLR1文法又添加了SLR1分析表，其主要数据结构如下。这非常好理解，不多做解释。

```C++
class SLR1Grammar : public LRGrammar
{
    ......
public:
    table_t<state_id_t, symbol_t, action_t> slr1Table;
    ......
};
```

除了上述继承链外，算符优先文法独树一帜。它直接继承自基础文法，其数据结构如下：

```C++
class OperatorPrecedenceGrammar : public Grammar
{
    ......
public:
    std::map<symbol_t, symset_t> firstVT;
    std::map<symbol_t, symset_t> lastVT;
    table_t<symbol_t, symbol_t, int> opt; // operator precedence table
    ......
};
```



#### 2.4.2 经典语法分析算法的设计与实现

#### 2.4.3 扩展语法和扩展语法分析器的实现

扩展语法在这里并非指编译原理中扩展文法的概念。

### 2.5 三种解析语法树的构造转换

修枝剪叶，添加层数可以简化问题

#### 2.5.1 语法解析树PST抽象类的设计与实现

#### 2.5.2 在语法分析的过程中构建具象语法树CST

#### 2.5.3 删去CST中冗余信息得到简化语法树RST

#### 2.5.4 重构RST的语义结构得到抽象语法树AST



### 2.6 类LLVM IR的中间表示生成

#### 2.6.1 LLVM IR简介

LLVM IR（LLVM Intermediate Representation）是 LLVM （Low-Level Virtual Machine）编译器框架中的一种中间代码表示形式，它类似于汇编语言，但比汇编语言更高级，但比常见的编程语言更低级。

LLVM IR 的设计初衷是为了提供一种通用的中间代码表示形式，可以被多种编程语言的前端所使用，并且可以被编译成多种目标平台的机器码。LLVM IR 具有以下特点：

1. 类型安全：LLVM IR 代码中的每个值都有一个明确的类型，并且类型检查是在编译时进行的，可以帮助开发者避免类型错误。
2. 抽象：LLVM IR 代码中的指令和操作符可以看作是高级语言中的语法结构，可以方便地表示程序的逻辑结构和算法。
3. 可移植性：LLVM IR 代码可以被编译成多种目标平台的机器码，具有很好的可移植性和可扩展性。

在 LLVM 编译器框架中，LLVM IR 通常是前端将源代码编译成的中间代码表示形式，经过优化和转换后再生成目标平台的机器码。同时，LLVM IR 代码也可以手动编写，用于实现一些特殊的编译器优化或调试功能。

使用 LLVM IR 作为个人实现编译器的输出结果有以下优势：

1. 简单：LLVM IR 代码比目标平台的汇编语言更容易理解和调试，可以帮助开发者更快速地实现编译器的各个阶段。
2. 可移植：LLVM 编译器可以将 LLVM IR 代码编译成多种目标平台的机器码，可以帮助开发者更方便地测试和验证编译器的正确性和性能。
3. 高效：LLVM 编译器框架具有优秀的优化能力，可以将 LLVM IR 代码优化成更高效的机器码，从而提高程序的性能。

#### 2.6.2 RSC类型数据引用链体系设计

#### 2.6.3 符号表和分析上下文的设计

#### 2.6.4 LLVM Instruction设计与定义

#### 2.6.5 AST Visitor模式设计与实现

##### 2.6.5.1 算数和逻辑表达式的翻译

##### 2.6.5.2 函数声明、定义和调用的翻译

##### 2.6.5.3 流程控制语句的翻译

##### 2.6.5.4 立即数、局部变量和全局变量

#### 2.6.6 未完待续

### 2.7 面向NASM的目标代码生成

#### 2.7.1 未完待续

### 2.8 自主编写的辅助开发工具集

#### 2.8.1 格式化日志输出工具设计与实现

#### 2.8.2 自动表格排版工具设计与实现

#### 2.8.3 输入流浏览工具设计与实现

#### 2.8.4 项目单元测试流程

## 3 运行结果

3.1 

## 4 相关记录

### 4.1 代码统计

核心代码 注释率

说明文档

配置文件

测试样例

### 4.2 提交记录

### 4.3 删改记录

## 5 代码说明

### 5.1 代码开源

### 5.2 文件结构