# 专题二设计

[TOC]

### 前言

由于md对latex公式渲染和代码排版和高亮良好支持，我还是选用了md作为实验报告书写格式。并附上了实验报告的封面，将其导出成pdf和word。

`Satori`意味**开悟、顿悟**，我希望能够通过一系列的动手实践让我在学习的过程中收获包括技术和人生方面的感悟，因而为自己设计的编译器冠以此名。

### 一、目标任务

完成以下描述赋值语句的 LL(1)文法的递归下降分析程序：

 G[S]: S→V=E E→TE’ E’→ATE’|ε T→FT’ T’→MFT’|ε F→(E)|i A→+|- M→*|/ V→i 

设计递归下降分析程序，能够发现简单的语法错误，设计测试用例给出测试结果。

### 二、设计说明

其实本次实验的本意是让我们针对**特定文法**的**每一个非终结符**设计一个可以递归调用的解析函数，并以此来实现符合文法的输入串的解析。然而，为了挑战自己，我效仿现代语法分析器生成器的经验，首先实现针对**扩展的巴克斯淖尔范式的文法的解析**，而后实现**针对任意文法的递归下降解析器**。

### 三、实验步骤

#### 3.1 扩展的BNF描述文法的解析

##### 3.1.1 文法解析格式的定义

我将解析形如以下格式（`.stx`）的文法定义：

```C++
GRAMMAR ${
    Stmt*       ::=     `if` Exp `then` Stmt [`else` Stmt]
                        | `while` Exp `do` Stmt
                        | `begin` Block `end`
                        | StmtItem ;
    Block       ::=     Stmt { `;` Stmt } ;
    StmtItem    ::=     $LValue `=` Exp ;
    Exp         ::=     ExpItem AddItem ;
    AddItem     ::=     (`+` | `-`) ExpItem AddItem | \e ;
    ExpItem     ::=     Factor MulItem ;
    MulItem     ::=     (`*` | `/`) Factor MulItem | \e ;
    Factor      ::=     `(`Exp`)` | $Num ;
$}

MAPPING ${
    $LValue     -->     @IDENTIFIER ;
    $Num        -->     @REAL | @INTEGER ;
$}
```

其中，`GRAMMAR`标记块指示了使用EBNF标记的文法，而`MAPPING`标记块则指示了词法解析结果到文法指定的终结符的映射，下面会解释具体原因。

##### 3.1.2 针对文法定义的词法设计

我利用自己实现的词法分析器来首先将EBNF解析成token序列，以便后续的解析。定义针对文法的词法如下：

```
PATTERN ${
    BLANK       \s+
    GRAMMAR     GRAMMAR
    MAPPING     MAPPING
    EPSILON     \\e
    START_MRK   \*
    BLOCK_SRT   $\{
    BLOCK_END   $\}
    TERMINAL    `[^`]*`
    NON_TERM    [\a_][\w']*
    MUL_TERM    $[\a_][\w']*
    TOK_TYPE    @[\a_][\w']*
    DELIMITER   [\(\){}\[\]\|]
    SEPARATOR   ;
    GRAMMAR_DEF ::=
    MAPPING_DEF -->
    COMMENT     //[^\r\n]*
$}

IGNORED ${
    BLANK
    COMMENT
$}
```

##### 3.1.3 递归下降的EBNF解析器

EBNF解析器结构如下：

```C++
typedef pair<symbol_t, vector<token>> tok_product_t;

class EBNFParser
{
    Lexer ebnfLexer;
    vector<token> tokens;
    Grammar grammar;
    void tokenizeSyntax(string grammarPath);
    vector<tok_product_t> segmentProduct(tok_product_t product);
    vector<tok_product_t> geneStxProducts(token_iter_t start, token_iter_t end);
    vector<tok_product_t> geneMapProducts(token_iter_t start, token_iter_t end);
    void addRules(vector<tok_product_t> &products);
    void addMappings(vector<tok_product_t> &products);

public:
    EBNFParser(string ebnfLexPath);
    Lexer &getLexer() { return ebnfLexer; }
    Grammar parse(string grammarPath);
};
```

其中最为核心的一个方法便是`segmentProduct`，该方法递归地拆分EBNF中的元素，并进行解析和产生式生成：

```C++
vector<tok_product_t> EBNFParser::segmentProduct(tok_product_t product)
{
    info << "EBNFParser: Segmenting product: " << product.first << " -> ";
    for (auto it = product.second.begin(); it != product.second.end(); it++)
    {
        cout << it->value << " ";
    }
    cout << endl;
    vector<tok_product_t> products;
    symbol_t &left = product.first;
    vector<token> &right = product.second;
    auto lastIt = right.begin();
    auto nextIt = right.begin();
    auto deliBeginIt = findType(right, get_tok_type("DELIMITER"), right.begin());
    if (deliBeginIt == right.end())
    {
        products.push_back(product);
        info << "EBNFParser: No delimiter found, return directly." << endl;
        return products;
    }
    while (lastIt != right.end())
    {
        if (deliBeginIt == right.end()) // 无分隔符
        {
            products.push_back(make_pair(left, vector<token>(lastIt, right.end())));
            break;
        }
        else if (deliBeginIt->value == "|")
        {
            // 如果是选择符，则把左边的产生式直接加入到结果中
            products.push_back(make_pair(left, vector<token>(lastIt, deliBeginIt)));
            nextIt = deliBeginIt;
        }
        else
        {
            // 如果是分组符，则需要递归拆分
            symbol_t endDeli = getEndSep(deliBeginIt->value);
            auto deliEndIt = findSep(right, endDeli, deliBeginIt);
            assert(
                deliEndIt != right.end(),
                format(
                    "EBNFParser: EBNF syntax error: Expected $, got EOF at <$, $>",
                    endDeli, right.back().line, right.back().col));
            nextIt = findType(right, get_tok_type("DELIMITER"), deliEndIt + 1);
            assert( // 如果分组后面有非选择符的分隔符，则报错（不支持多种分隔符排列组合）
                nextIt == right.end() || nextIt->value == "|",
                format(
                    "EBNFParser: EBNF syntax error: Expected |, got $ at <$, $>",
                    nextIt->value, nextIt->line, nextIt->col));
            vector<tok_product_t> subProducts;
            subProducts = segmentProduct(make_pair(left, vector<token>(deliBeginIt + 1, deliEndIt)));
            vector<token> head = vector<token>(lastIt, deliBeginIt);
            vector<token> tail = vector<token>(deliEndIt + 1, nextIt);
            if (deliBeginIt->value == "{")
            {
                // S -> A{B|D}C => S -> AB'C, B' -> B | ε, S -> AD'C, D' -> D | ε
                int nonTermCount = 0;
                for (auto &pro : subProducts)
                {
                    // 构造新的非终结符
                    symbol_t newLeft = left + "_";
                    newLeft += to_string(++nonTermCount);
                    token newLeftTok = token(get_tok_type("NON_TERM"), newLeft);
                    // 构造含新非终结符的新产生式
                    vector<token> newRight;
                    newRight.insert(newRight.end(), head.begin(), head.end());
                    newRight.push_back(newLeftTok);
                    newRight.insert(newRight.end(), tail.begin(), tail.end());
                    products.push_back(make_pair(left, newRight));
                    // 构造含右递归的新产生式
                    vector<token> subRight = pro.second;
                    subRight.push_back(newLeftTok);
                    products.push_back(make_pair(newLeft, subRight));
                    // 构造含空串的新产生式
                    products.push_back(make_pair(newLeft, vector<token>()));
                }
            }
            else if (deliBeginIt->value == "(" || deliBeginIt->value == "[")
            {
                // S -> A[B|D]C => S -> ABC, S -> ADC, S -> AC
                // S -> A(B|D)C => S -> ABC, S -> ADC
                for (auto &pro : subProducts)
                {
                    // S -> ABC, S -> ADC
                    vector<token> newRight;
                    newRight.insert(newRight.end(), head.begin(), head.end());
                    newRight.insert(newRight.end(), pro.second.begin(), pro.second.end());
                    newRight.insert(newRight.end(), tail.begin(), tail.end());
                    products.push_back(make_pair(left, newRight));
                }
                if (deliBeginIt->value == "[")
                {
                    // S -> AC
                    vector<token> newRight;
                    newRight.insert(newRight.end(), head.begin(), head.end());
                    newRight.insert(newRight.end(), tail.begin(), tail.end());
                    products.push_back(make_pair(left, newRight));
                }
            }
            else
            {
                error << "EBNFParser: EBNF syntax error: Expected (, [, or { ." << endl;
                exit(1);
            }
        }
        if (nextIt == right.end())
        {
            break;
        }
        lastIt = nextIt + 1;
        info << "EBNFParser: lastIt: " << lastIt->value << endl;
        if (nextIt != right.end())
        {
            deliBeginIt = findType(right, get_tok_type("DELIMITER"), lastIt);
        }
    }
    info << "EBNFParser: Segmented product: " << endl;
    for (auto &pro : products)
    {
        cout << pro.first << " -> ";
        for (auto &tok : pro.second)
        {
            cout << tok.value << " ";
        }
        cout << endl;
    }
    return products;
}
```

#### 3.2 预测文法的数据和算法描述

我实现了预测文法（即自动计算预测分析表所需的first、follow集的文法，并进行了左公因子的提取了左递归的消除）：

首先给出基本的文法定义（基类）：

```C++
typedef string symbol_t;
typedef set<symbol_t> symset_t;
typedef vector<symbol_t> symstr_t;
typedef pair<symbol_t, symstr_t> product_t;

class Grammar
{

public:
    symbol_t symStart;
    symset_t nonTerms;
    symset_t terminals;
    vector<product_t> products;
    map<symbol_t, set<symstr_t>> rules;
    map<token_type_t, symbol_t> tok2sym;

    Grammar(symbol_t start, symset_t terms, symset_t nonTerms, vector<product_t> products, map<symbol_t, set<symstr_t>> rules, map<token_type_t, symbol_t> tok2sym);
    Grammar() { terminals.insert(SYM_END); }
    Grammar(const Grammar &g)
    {
        symStart = g.symStart;
        terminals = g.terminals;
        nonTerms = g.nonTerms;
        tok2sym = g.tok2sym;
        products = g.products;
        rules = g.rules;
    }
    void eliminateLeftRecursion();
    void extractLeftCommonFactor();
    void printRules();
    void printTerminals();
    void printNonTerms();
    vector<token> transferTokens(vector<token> tokens);
};
```

该文法实现了左公因子的提取，在算法的实现上，我发现如果仅仅是通过循环的话不仅效率很低，总体的去除效果也不是很好，于是便采用了字典树的方法。通过首先将已有的产生式构造成一棵字典树，然后再递归地尝试提取、重构具有多个子树的节点，以此来达到消除左公因子的目的：

```C++
void Grammar::extractLeftCommonFactor()
{
    info << "Grammar: Extracting Left Common Factor" << endl;
    // 将规则按照左部分组，分别构造TermTree
    map<symbol_t, tt_node_ptr_t> termTrees;
    for (auto &A : nonTerms)
    {
        termTrees[A] = tt_node_t::createNode(A);
        for (auto alphaIt = rules[A].begin(); alphaIt != rules[A].end(); alphaIt++)
        {
            auto &alpha = *alphaIt;
            tt_node_ptr_t cur = termTrees[A];
            for (size_t i = 0; i < alpha.size(); i++)
            {
                auto &sym = alpha[i];
                size_t index = cur->find(sym);
                if (index == -1)
                {
                    tt_node_ptr_t new_node = tt_node_t::createNode(sym, alphaIt, i);
                    (*cur) << new_node;
                    cur = new_node;
                }
                else
                {
                    cur = cur->get_child_ptr(index);
                }
            }
            (*cur) << tt_node_t::createNode("#", alphaIt, 0);
        }
        termTrees[A]->print();
    }
    // 递归遍历TermTree，修正规则集合
    info << "Grammar: Fixing Rules" << endl;
    size_t cnt = 0;
    for (auto &tree : termTrees)
    {
        const symbol_t &left = tree.first;
        tt_node_ptr_t &root = tree.second;
        root->postorder(
            [&](tt_node_t node)
            {
                if (node.size() > 1 && node.data.index != 0)
                {
                    // 有公共前缀
                    info << "Grammar: Fixing Rule " << left << "->" << node.data.symbol;
                    cout << " " << node.data.index << endl;
                    assert(node.data.ruleIt != rules[left].end());
                    symstr_t curRule = *(node.data.ruleIt);
                    symstr_t prefix;
                    prefix.insert(prefix.end(), curRule.begin(), curRule.begin() + node.data.index + 1);
                    symbol_t newNonTerm = left + "^" + to_string(++cnt);
                    nonTerms.insert(newNonTerm);
                    prefix.push_back(newNonTerm);
                    rules[left].insert(prefix);
                    node.foreach (
                        [&](tt_node_t child)
                        {
                            // 对于每个子节点，将其所代表的子产生式加入新的规则
                            symstr_t suffix;
                            if (child.data.symbol != "#")
                            {
                                suffix.insert(
                                    suffix.end(),
                                    child.data.ruleIt->begin() + child.data.index,
                                    child.data.ruleIt->end());
                            }
                            rules[newNonTerm].insert(suffix);
                            rules[left].erase(child.data.ruleIt);
                        });
                }
            });
    }
}
```

为了支持上述算法，我额外实现了抽象的树结构并继承之得到了一个用于提取公共因子的符号树：

```C++
class TermTreeNode;
typedef TermTreeNode tt_node_t;
typedef shared_ptr<TermTreeNode> tt_node_ptr_t;
typedef vector<tt_node_ptr_t> tt_children_t;

struct tt_node_data
{
    symbol_t symbol;
    set<symstr_t>::iterator ruleIt;
    size_t index;
};

class TermTreeNode : public AbstractTreeNode<tt_node_data>
{
public:
    TermTreeNode(tt_node_data t) : AbstractTreeNode<tt_node_data>(t) {}

    static tt_node_ptr_t createNode(symbol_t symbol, set<symstr_t>::iterator ruleIt, size_t index)
    {
        return make_shared<TermTreeNode>(tt_node_data(symbol, ruleIt, index));
    }

    static tt_node_ptr_t createNode(symbol_t symbol)
    {
        return make_shared<TermTreeNode>(tt_node_data(symbol, set<symstr_t>::iterator(), 0));
    }

    size_t find(symbol_t symbol) const
    {
        auto cmp = [=](tree_node_ptr_t<tt_node_data> ptr)
        {
            tt_node_ptr_t node = static_pointer_cast<TermTreeNode>(ptr);
            return node->data.symbol == symbol;
        };
        return AbstractTreeNode<tt_node_data>::find(cmp);
    }

    TermTreeNode &operator<<(const tt_node_ptr_t node)
    {
        return static_cast<TermTreeNode &>(AbstractTreeNode<tt_node_data>::operator<<(
            static_pointer_cast<AbstractTreeNode<tt_node_data>>(node)));
    }

    TermTreeNode &operator[](size_t index)
    {
        return static_cast<TermTreeNode &>(AbstractTreeNode<tt_node_data>::operator[](index));
    }

    tt_node_ptr_t get_child_ptr(size_t index) const
    {
        return static_pointer_cast<TermTreeNode>(AbstractTreeNode<tt_node_data>::get_child_ptr(index));
    }

    template <typename func_t>
    void foreach (func_t f) const
    {
        auto nodeF = [=](tree_childs<tt_node_data>::const_reference ref)
        {
            f(static_cast<TermTreeNode &>(*(ref)));
        };
        for_each(this->begin(), this->end(), nodeF);
    }

    template <typename func_t>
    void postorder(func_t f) const
    {
        foreach (
            [=](tt_node_t &ref)
            { ref.postorder(f); })
            ;
        f(*this);
    }

    string desc() const override
    {
        stringstream ss;
        ss << data.symbol << " " << data.index;
        return ss.str();
    }
};
```

考虑到篇幅因素，不再做展开讲解。

在上面的基础上，构造预测文法，计算预测各集合：

```C++
class PredictiveGrammar : public Grammar
{
protected:
    symset_t calcFirstOf(symbol_t t);
    symset_t calcFirstOf(symstr_t s);
    symset_t calcFollowOf(symbol_t t);
    symset_t calcSelectOf(product_t p);
    void calcFirst();
    void calcFollow();
    void calcSelect();

public:
    map<symbol_t, symset_t> first;
    map<symstr_t, symset_t> firstS;
    map<symbol_t, symset_t> follow;
    map<product_t, symset_t> select;
    PredictiveGrammar() : Grammar(){};
    PredictiveGrammar(const Grammar &g) : Grammar(g)
    {
        calcFirst();
        calcFollow();
        calcSelect();
    }
    PredictiveGrammar(const PredictiveGrammar &g) : Grammar(g)
    {
        first = g.first;
        firstS = g.firstS;
        follow = g.follow;
        select = g.select;
    }
    void printFirst();
    void printFollow();
    void printFirstS();
    void printSelect();
    bool isLL1Grammar();
};
```

其中计算各类集合的方法如下：

```C++
symset_t PredictiveGrammar::calcFirstOf(symbol_t t)
{
    debug(0) << "Calculating First(" << t << ")" << endl;
    if (first[t].size() > 0)
    {
        debug(1) << "First(" << t << ") has been calculated before" << endl;
        return first[t]; // 已经计算过
    }
    // 对于每个产生式，计算first集
    for (auto &right : rules[t])
    {
        // 空产生式，加入epsilon
        if (right.size() == 0)
        {
            first[t].insert(EPSILON);
            debug(1) << "First(" << t << ") <- {epsilon}" << endl;
        }
        else
        {
            // 对于产生式右部的每个符号
            bool allHaveEpsilon = true;
            for (auto it = right.begin(); it != right.end(); it++)
            {
                if (_find(terminals, *it))
                {
                    // 终结符，直接加入first集
                    first[t].insert(*it);
                    debug(1) << "First(" << t << ") <- {" << *it << "}" << endl;
                    allHaveEpsilon = false;
                    break; // 终结符后面的符号不再计算
                }
                else
                {
                    assert(it != right.begin() || *it != t, "Direct left recursion detected.");
                    symset_t resFirst = calcFirstOf(*it);
                    symset_t tmpFirst = resFirst;
                    tmpFirst.erase(EPSILON);
                    first[t].insert(tmpFirst.begin(), tmpFirst.end());
                    for (auto &f : tmpFirst)
                    {
                        debug(1) << "First(" << t << ") <- {" << f << "}" << endl;
                    }
                    if (!_find(resFirst, EPSILON))
                    {
                        // 该符号的first集不含epsilon，后面的符号不再计算
                        allHaveEpsilon = false;
                        break;
                    }
                }
            }
            if (allHaveEpsilon)
            {
                first[t].insert(EPSILON);
                debug(1) << "First(" << t << ") <- {epsilon}" << endl;
            }
        }
    }
    return first[t];
}

symset_t PredictiveGrammar::calcFirstOf(symstr_t s)
{
    debug(0) << format("Calculating First($)", vec2str(s)) << endl;
    if (firstS[s].size() > 0)
    {
        debug(1) << format(
            "First($) has been calculated before.\n",
            vec2str(s));
        return firstS[s]; // 已经计算过
    }
    symset_t resFirst;
    bool allHaveEpsilon = true;
    for (auto &symbol : s)
    {
        if (_find(terminals, symbol))
        {
            // 终结符，直接加入first集
            resFirst.insert(symbol);
            debug(1) << "First(" << vec2str(s) << ") <- {" << symbol << "}" << endl;
            allHaveEpsilon = false;
            break; // 终结符后面的符号不再计算
        }
        else
        {
            symset_t tmpFirst = calcFirstOf(symbol);
            resFirst.insert(tmpFirst.begin(), tmpFirst.end());
            debug(1) << "First(" << vec2str(s) << ") <- " << set2str(tmpFirst) << endl;
            if (!_find(tmpFirst, EPSILON))
            {
                // 该符号的first集不含epsilon，后面的符号不再计算
                allHaveEpsilon = false;
                break;
            }
        }
    }
    if (allHaveEpsilon)
    {
        resFirst.insert(EPSILON);
        debug(1) << "First(" << vec2str(s) << ") <- { epsilon }" << endl;
    }
    firstS[s] = resFirst;
    return resFirst;
}

symset_t PredictiveGrammar::calcFollowOf(symbol_t t)
{
    debug(0) << "Calculating Follow(" << t << ")" << endl;
    if (follow[t].size() > 0)
    {
        debug(1) << "Follow(" << t << ") has been calculated before" << endl;
        return follow[t]; // 已经计算过
    }
    if (t == symStart)
    {
        follow[t].insert(SYM_END);
        debug(1) << "Follow(" << t << ") <- {#}" << endl;
    }
    for (auto &rule : rules)
    {
        for (auto &right : rule.second)
        {
            auto symIt = right.begin();
            symIt = find(symIt, right.end(), t);
            while (symIt != right.end())
            {
                if (symIt == right.end() - 1)
                {
                    // A -> aB
                    if (rule.first != t)
                    {
                        symset_t resFollow = calcFollowOf(rule.first);
                        follow[t].insert(resFollow.begin(), resFollow.end());
                        for (auto &f : resFollow)
                        {
                            debug(1) << "Follow(" << t << ") <-> {" << f << "} <A -> aB>" << endl;
                        }
                    }
                }
                else if (_find(terminals, *(symIt + 1)))
                {
                    // A -> aBb
                    follow[t].insert(*(symIt + 1));
                    debug(1) << "Follow(" << t << ") <- {" << *(symIt + 1) << "} <A -> aBb>" << endl;
                }
                else
                {
                    // A -> aBC
                    symset_t resFirst;
                    for (auto tmpIt = symIt + 1; tmpIt != right.end(); tmpIt++)
                    {
                        resFirst = calcFirstOf(*tmpIt);
                        symset_t tmpFirst = resFirst;
                        tmpFirst.erase(EPSILON);
                        follow[t].insert(tmpFirst.begin(), tmpFirst.end());
                        for (auto &f : tmpFirst)
                        {
                            debug(1) << "Follow(" << t << ") = {" << f << "} <A -> aBC>" << endl;
                        }
                        if (!_find(resFirst, EPSILON))
                        {
                            break;
                        }
                    }
                    if (resFirst.size() == 0 || _find(resFirst, EPSILON))
                    {
                        // A -> aBC, First(C) 含有epsilon
                        if (rule.first != t)
                        {
                            symset_t resFollow = calcFollowOf(rule.first);
                            follow[t].insert(resFollow.begin(), resFollow.end());
                            for (auto &f : resFollow)
                            {
                                debug(1) << "Follow(" << t << ") = {" << f << "} <A -> aBC(e)>" << endl;
                            }
                        }
                    }
                }
                symIt = find(symIt + 1, right.end(), t);
            }
        }
    }
    return follow[t];
}

symset_t PredictiveGrammar::calcSelectOf(product_t p)
{
    debug(0) << "Calculating Select(" << p.first;
    debug_u(0) << " -> " << vec2str(p.second) << ")" << endl;
    symset_t resSelect;
    symset_t resFirst = firstS[p.second];
    symset_t tmpFirst = resFirst;
    tmpFirst.erase(EPSILON); // First(A) - {epsilon}
    resSelect.insert(tmpFirst.begin(), tmpFirst.end());
    debug(1) << "Select(" << p.first << " -> " << vec2str(p.second);
    debug_u(1) << ") <- " << set2str(tmpFirst) << endl;
    if (_find(resFirst, EPSILON))
    {
        symset_t resFollow = calcFollowOf(p.first);
        resSelect.insert(resFollow.begin(), resFollow.end());
        debug(1) << "Select(" << p.first << " -> " << vec2str(p.second);
        debug_u(1) << ") <- " << set2str(resFollow) << endl;
    }
    return resSelect;
}
```

#### 3.3 词法结果到文法的衔接映射

由于词法定义和文法定义是分开的，而且为了提高抽象程度他们并没有被直接在代码中实现，所以如何实现词法分析器和语法分析器的协调配合是必须考虑的内容。

这样的考虑是有现实因素的。举例来说，标识符类型是词法分析中最为常见的词法类型，然而，文法定义却不接受这样的类型。原因也很好理解，在文法的世界，只有终结符和非终结符之说，而与具体的原文件对应的就只有终结符了。那么请问标识符应该怎样和终结符对应起来呢？在老师给的实验要求中，这个问题非常简单：一律使用`i`代表终结符和标识符就可以了，这样词法分析和语法分析程序无需做额外的处理便可顺利地跑起来，而这在实际生产中显然是不行的。

事实上，文法中的终结符很多时候就是关键字，如果词法分析器解析到标识符类型（以字母或下划线开头，可能包含数字的词素），则要首先判断该词素的值是否是文法中的某个终结符，也就是关键字，如果不是，则将其统统归类为标识符类型，并存储其值，留给后续的语义分析阶段进行处理。

通过以上分析，我们发现词法分析器的输出结果并不能直接喂给语法分析器，而要经过一步翻译（或者说是映射）才可以。总结一下，就是需要将词法分析结果的二元式中的词法类型翻译为文法定义中的终结符，而词素的值保持不变。

这一点我直接在文法定义中进行了实现，相关实现代码如下：

```C++
vector<token> Grammar::transferTokens(vector<token> tokens)
{
    info << "Transferring tokens..." << endl;
    vector<token> res;
    for (auto &t : tokens)
    {
        if (_find(terminals, t.value))
        {
            debug(0) << "terminals: " << t.value << endl;
            res.push_back(token(
                make_shared<symbol_t>(t.value),
                t.value,
                t.line,
                t.col));
        }
        else if (_find(tok2sym, t.type))
        {
            debug(0) << "tok2sym: " << t.value << endl;
            res.push_back(
                token(
                    make_shared<symbol_t>(tok2sym[t.type]),
                    t.value,
                    t.line,
                    t.col));
        }
        else
        {
            res.push_back(t);
            warn << "Grammar::transferTokens: Unknown token: " << t.value << endl;
        }
    }
    return res;
}
```

#### 3.4 递归下降解析器的设计实现

经过一番周折，终于才来到了本次实验最初的目的……

首先给出解析器的定义：

```C++
class PredictiveRecursiveDescentParser
{
    PredictiveGrammar grammar;
    cst_tree_ptr_t tree;
    bool parseNonTerm(TokenViewer &viewer, symbol_t sym, cst_node_ptr_t node);

public:
    PredictiveRecursiveDescentParser(PredictiveGrammar &grammar) : grammar(grammar)
    {
        tree = cst_tree_t::createNode(TERMINAL, SYM_END, 0, 0);
    }
    bool parse(vector<token> &input);
    cst_tree_ptr_t getTree() { return tree; }
};

typedef PredictiveRecursiveDescentParser PRDParser;
```

该解析器拥有一个预测文法、一颗具象语法树的根节点以及核心的解析方法。

我实现了自动根据文法的Follow集和First集构造递归下降函数的函数，其核心实现如下：

```C++
bool PredictiveRecursiveDescentParser::parseNonTerm(TokenViewer &viewer, symbol_t sym, cst_node_ptr_t node)
{
    for (auto &right : grammar.rules[sym])
    {
        if (right.size() == 0)
        {
            // 空产生式，即epsilon
            debug(0) << format("Viewing null production: $ -> $\n", sym, compact(right));
            symset_t &follow = grammar.follow[sym];
            debug(0) << format("Follow set: $.\n", set2str(follow));
            if (_find(follow, _cur_tok(viewer)))
            {
                // 分析成功
                token &tok = viewer.current();
                *(node) << cst_tree_t::createNode(TERMINAL, EPSILON, tok.line, tok.col);
                return true;
            }
        }
        else
        {
            symset_t &first = grammar.firstS[right];
            if (_find(first, _cur_tok(viewer)))
            {
                debug(0) << format("Viewing production: $ -> $\n", sym, compact(right));
                debug(0) << format("First set: $.\n", set2str(first));
                // 根据first集预测选择唯一一个产生式，若分析失败不再考察其他产生式
                for (auto symIt = right.begin(); symIt != right.end(); symIt++)
                {
                    token &tok = viewer.current();
                    debug(0) << format("Viewer currently focuses on: $.\n", tok.value);
                    if (_find(grammar.terminals, *symIt))
                    {
                        // 终结符
                        if (_cur_tok(viewer) != *symIt)
                        {
                            error << "PRDParser: parseNonTerm: terminal not match."
                                  << format(" Expected: $, got: $.\n", *symIt, _cur_tok(viewer));
                            return false; // 分析失败
                        }
                        *(node) << cst_tree_t::createNode(TERMINAL, viewer.current().value, tok.line, tok.col);
                        info << format("PRDParser: parseNonTerm: terminal matched: $.\n", *symIt);
                        viewer.advance();
                        debug(0) << format("Viewer advanced to: $.\n", viewer.current().value);
                    }
                    else
                    {
                        // 非终结符
                        cst_node_ptr_t child = cst_tree_t::createNode(NON_TERM, *symIt, tok.line, tok.col);
                        *(node) << child;
                        info << format("PRDParser: parseNonTerm: goto nonTerm: $.\n", *symIt);
                        if (!parseNonTerm(viewer, *symIt, child))
                        {
                            error << "PRDParser: parseNonTerm: non-terminal not match."
                                  << format(" Expected: $, got: $.\n", *symIt, _cur_tok(viewer));
                            return false; // 分析失败
                        }
                    }
                }
                return true;
            }
        }
    }
    error << "PRDParser: parseNonTerm: no production matched." << endl;
    return false;
}
```

该函数在完成解析的同时会构造输入串的具象语法树，若遇到失败则直接终止程序。

### 四、测试用例

我使用的测试主函数如下：

```C++
void prdTest()
{
    EBNFParser ebnfParser("./assets/syntax.lex");
    Grammar g = ebnfParser.parse("./assets/rlcf.stx");
    PredictiveGrammar G = PredictiveGrammar(g);
    G.printRules();
    G.extractLeftCommonFactor();
    G.printRules();
    G.printTerminals();
    G.printNonTerms();
    G.printFirst();
    G.printFollow();
    G.printFirstS();
    G.printSelect();
    cout << G.isLL1Grammar() << endl;
    PredictiveRecursiveDescentParser prd(G);
    Lexer lexer("./assets/lab3.lex");
    auto tokens = lexer.tokenizeFile("./assets/lab3.txt");
    Lexer::printTokens(tokens);
    tokens = G.transferTokens(tokens);
    Lexer::printTokens(tokens);
    info << "result: \n"
         << prd.parse(tokens) << endl;
    prd.getTree()->print();
}
```

此次实验采用的文法：

```C++
GRAMMAR ${
    Program*    ::=     StmtList ;
    StmtList    ::=     Stmt { `;` Stmt } ;
    Stmt        ::=     `if` Exp `then` Stmt [`else` Stmt]
                    | `while` Exp `do` Stmt
                    | `begin` Block `end`
                    | StmtItem ;
    Block       ::=     StmtList ;
    StmtItem    ::=     $LValue `=` Exp ;
    Exp         ::=     ExpItem AddItem ;
    AddItem     ::=     (`+` | `-`) ExpItem AddItem | \e ;
    ExpItem     ::=     Factor MulItem ;
    MulItem     ::=     (`*` | `/`) Factor MulItem | \e ;
    Factor      ::=     `(`Exp`)` | $Num | $LValue ;
    //FuncCall    ::=     $FuncName `(` [Exp { `,` Exp }] `)` ;
$}

MAPPING ${
    $LValue     -->     @IDENTIFIER ;
    $Num        -->     @REAL | @INTEGER ;
    //$FuncName   -->     @IDENTIFIER ;
$}
```

此次实验采用的词法：

```
PATTERN ${
    BLANK       \s+
    LIN_CMT     //[^\r\n]*
    IDENTIFIER  [\a_][\w]*
    SEPARATOR   [\+\-\*/=\(\)]
$}

IGNORED ${
    BLANK
    LIN_CMT
$}
```

此次实验使用的测试源程序：

```C++
res=a*b+(c/d-e)
//res=a*b+(c/d-
//=a*b+(c/d-
```

首先，使用扩展的巴克斯淖尔范式解析器解析我定义的文法，而后在基础文法的基础上得到预测文法，再将预测文法传递给递归下降解析器，至此便完成了初始的配置。而后设置一个词法解析器，用其解析输入串，并将词素序列传递给文法解析器完成分析，最后打印出具象语法树。

上述流程的执行结果如下：

扩展的巴克斯淖尔范式解析器解析文法得到的词素序列：

![image-20230526002317529](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002317529.png)

EBNF解析器正在递归下降地解析文法并生成产生式：

![image-20230526002405935](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002405935.png)

![image-20230526002414898](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002414898.png)

基础文法正在尝试使用字典树法提取左公因子：

![image-20230526002526843](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002526843.png)

最终得到产生式如下：

![image-20230526002600085](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002600085.png)

预测文法进一步计算First集、Follow集：

![image-20230526002626697](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002626697.png)

顺便计算出了针对产生式右部符号串的First集：

![image-20230526002700773](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002700773.png)

针对源语言的词法分析器开始初始化，并完成源语言的词法解析：

![image-20230526002752838](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002752838.png)

词法输出到文法输入的翻译过程：

![image-20230526002823000](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002823000.png)

递归下降解析过程：

![image-20230526002847312](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002847312.png)

最终生成具象语法树（部分）：

![image-20230526002910252](D:\CodeBase\C++Prjs\SatoriCompiler\docs\assets\image-20230526002910252.png)

我并未进行详细的错误处理，如果出错程序会直接终止，因此不再进行演示。

### 五、实验总结

本次专题实验，我实现了扩展的巴克斯淖尔范式的解析程序，并进行了词素翻译，最终使用自动生成递归下降分析函数的函数完成了递归下降的解析。

### 六、代码说明

#### 6.1 代码开源

本次实验及后续实验所涉及到的代码均共享同一个项目框架，且源码开源在Github。

Github开源链接：https://github.com/Kogler7/SatoriCompiler

#### 6.2 代码结构

根目录

- `assets`：资源文件夹，例如测试用的源代码，以及词法分析器、语法分析器（目前还没有）的配置文件
- `docs`：说明文档，目前主要放实验报告及相关截图
- `src`：源代码文件夹
  - `codegen`：目标代码生成模块（暂时还没有）
  - `lexer`：词法分析器模块（本次实验主角）
  - `parser`：语法分析器模块（暂时还没有）
  - `utils`：模块间共享的一些工具模块
  - `main.cpp`：程序主入口文件
- `CMakeList`：CMake说明
- `LICENSE`：开源协议（MIT）
- `README.md`：其他说明
